# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PBn_1ZCTKmuCMjKsfISpzvr3BJfxvjuT
"""

import os
import sys
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
from pathlib import Path
import joblib
import json
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, log_loss, accuracy_score, classification_report
from xgboost import XGBClassifier
!pip install lime
from lime.lime_tabular import LimeTabularExplainer
# -----------------------------
# User settings
# -----------------------------
DATA_PATH = "/default of credit card clients.xls"
OUTPUT_DIR = "project_outputs"
SAMPLE_FOR_SHAP = 2000

Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)

# -----------------------------
# Utility functions
# -----------------------------
def load_dataset(path):
    """Load CSV or Excel; find target column among common names."""
    if not os.path.exists(path):
        raise FileNotFoundError(f"Dataset not found at {path}. Update DATA_PATH.")
    ext = Path(path).suffix.lower()
    if ext in [".csv", ".txt"]:
        df = pd.read_csv(path)
    elif ext in [".xls", ".xlsx"]:
        df = pd.read_excel(path)
    else:
        raise ValueError("Unsupported extension. Use CSV or Excel.")
    # Clean column names
    df.columns = df.columns.str.strip()
    # Detect target column
    if "Y" in df.columns:
        target = "Y"
    elif "default" in df.columns:
        target = "default"
    elif "target" in df.columns:
        target = "target"
    else:
        # heuristics
        candidates = [c for c in df.columns if any(k in c.lower() for k in ["def", "target", "y"])]
        if candidates:
            target = candidates[0]
        else:
            raise ValueError("Could not find target column. Name a column 'Y' or 'default' or 'target'.")
    return df, target

def coerce_numeric(df, exclude=None):
    exclude = exclude or []
    for c in df.columns:
        if c in exclude: continue
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df

def save_text(path, text):
    with open(path, "w", encoding="utf-8") as f:
        f.write(text)

def ensure_dir(path):
    Path(path).mkdir(parents=True, exist_ok=True)

# -----------------------------
# Main pipeline
# -----------------------------
def main():
    print("Loading dataset from:", DATA_PATH)
    df, target = load_dataset(DATA_PATH)
    print("Detected target column:", target)
    print("Initial shape:", df.shape)

    # Standardize target values to 0/1
    df[target] = df[target].astype(str).str.strip()
    df[target] = df[target].replace({
        '0': 0, '1': 1, '2': 1,
        'Yes': 1, 'YES': 1, 'yes': 1,
        'No': 0, 'NO': 0, 'no': 0,
        'Default': 1, 'default': 1
    })
    df[target] = pd.to_numeric(df[target], errors="coerce")
    df = df.dropna(subset=[target]).reset_index(drop=True)
    df[target] = df[target].astype(int)

    # Convert features to numeric where possible
    features = [c for c in df.columns if c != target]
    df = coerce_numeric(df, exclude=[target])
    # Drop rows with NA in features (simplest policy; for production consider imputation)
    before_drop = df.shape[0]
    df = df.dropna(subset=features).reset_index(drop=True)
    after_drop = df.shape[0]
    print(f"Dropped {before_drop - after_drop} rows with missing features.")

    X = df[features].copy()
    y = df[target].copy()

    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42, stratify=y
    )
    print("Train shape:", X_train.shape, "Test shape:", X_test.shape)

    # -----------------------------
    # Model definitions & training
    # -----------------------------
    # Logistic Regression pipeline (scaler + L1 LR)
    lr_pipeline = Pipeline([
        ("scaler", StandardScaler()),
        ("clf", LogisticRegression(penalty="l1", solver="saga", max_iter=3000, C=1.0, random_state=42))
    ])
    print("Training Logistic Regression (L1)...")
    lr_pipeline.fit(X_train, y_train)

    print("Training XGBoost...")
    xgb = XGBClassifier(
        n_estimators=300,
        max_depth=5,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        use_label_encoder=False,
        eval_metric="logloss",
        random_state=42
    )
    xgb.fit(X_train, y_train)

    # Save models
    joblib.dump(lr_pipeline, os.path.join(OUTPUT_DIR, "logreg_pipeline.joblib"))
    joblib.dump(xgb, os.path.join(OUTPUT_DIR, "xgb.joblib"))

    # -----------------------------
    # Evaluation (metrics) and saving metrics_summary.txt
    # -----------------------------
    def evaluate_and_report(model, Xtr, Xte, yte, name, is_pipeline=False):
        if is_pipeline:
            prob = model.predict_proba(Xte)[:,1]
            pred = model.predict(Xte)
        else:
            prob = model.predict_proba(Xte)[:,1]
            pred = model.predict(Xte)
        auc = roc_auc_score(yte, prob)
        ll = log_loss(yte, prob)
        acc = accuracy_score(yte, pred)
        rep = classification_report(yte, pred)
        summary = {
            "name": name,
            "accuracy": float(acc),
            "auc": float(auc),
            "logloss": float(ll),
            "report": rep
        }
        return summary

    print("Evaluating models...")
    lr_summary = evaluate_and_report(lr_pipeline, X_train, X_test, y_test, "Logistic Regression (L1)", is_pipeline=True)
    xgb_summary = evaluate_and_report(xgb, X_train, X_test, y_test, "XGBoost", is_pipeline=False)

    # Build text metrics summary and save
    metrics_text_lines = []
    for s in [lr_summary, xgb_summary]:
        metrics_text_lines.append(f"=== Model: {s['name']} ===")
        metrics_text_lines.append(f"Accuracy: {s['accuracy']:.6f}")
        metrics_text_lines.append(f"AUC: {s['auc']:.6f}")
        metrics_text_lines.append(f"Log Loss: {s['logloss']:.6f}")
        metrics_text_lines.append("Classification Report:\n" + s['report'])
        metrics_text_lines.append("\n")

    metrics_text = "\n".join(metrics_text_lines)
    metrics_path = os.path.join(OUTPUT_DIR, "metrics_summary.txt")
    save_text(metrics_path, metrics_text)
    print("Saved metrics to:", metrics_path)
    print(metrics_text)

    # -----------------------------
    # SHAP: Global & Local (XGBoost)
    # -----------------------------
    print("Computing SHAP for XGBoost (TreeExplainer)...")
    try:
        explainer_xgb = shap.TreeExplainer(xgb)
    except Exception:
        explainer_xgb = shap.Explainer(xgb)

    # Compute shap values for a sample (for summary plot) and full test set (for locals)
    sample_for_shap = X_test if X_test.shape[0] <= SAMPLE_FOR_SHAP else X_test.sample(n=SAMPLE_FOR_SHAP, random_state=42)
    shap_vals_sample = explainer_xgb(sample_for_shap)
    # summary plot (save)
    plt.figure()
    shap.summary_plot(shap_vals_sample.values, sample_for_shap, show=False)
    shap_summary_path = os.path.join(OUTPUT_DIR, "shap_summary.png")
    plt.tight_layout()
    plt.savefig(shap_summary_path, bbox_inches="tight", dpi=200)
    plt.close()
    print("Saved SHAP summary plot to:", shap_summary_path)

    # Full SHAP values for test set (may be heavy)
    shap_vals_test = explainer_xgb(X_test)
    # compute mean abs importance
    try:
        shap_array = shap_vals_test.values
    except Exception:
        shap_array = np.array(shap_vals_test)  # fallback
    mean_abs_shap = np.abs(shap_array).mean(axis=0)
    shap_imp_df = pd.DataFrame({"feature": X_test.columns, "mean_abs_shap": mean_abs_shap})
    shap_imp_df = shap_imp_df.sort_values("mean_abs_shap", ascending=False).reset_index(drop=True)
    shap_imp_csv = os.path.join(OUTPUT_DIR, "shap_feature_importance.csv")
    shap_imp_df.to_csv(shap_imp_csv, index=False)
    print("Saved SHAP feature importance CSV to:", shap_imp_csv)
    top10_shap = shap_imp_df.head(10)["feature"].tolist()

    # -----------------------------
    # Logistic Regression global importance (coefficients) & SHAP for LR (if possible)
    # -----------------------------
    print("Extracting Logistic Regression coefficients and attempting SHAP LinearExplainer for LR...")
    # get feature names and coefficients (model inside pipeline)
    lr_model = lr_pipeline.named_steps['clf']
    scaler = lr_pipeline.named_steps['scaler']
    # coefficients correspond to scaled features; we will save the raw coefficient values as LR importance
    coef = lr_model.coef_.ravel()
    lr_imp_df = pd.DataFrame({"feature": X_train.columns, "coefficient": coef, "abs_coefficient": np.abs(coef)})
    lr_imp_df = lr_imp_df.sort_values("abs_coefficient", ascending=False).reset_index(drop=True)
    lr_imp_csv = os.path.join(OUTPUT_DIR, "logreg_feature_importance.csv")
    lr_imp_df.to_csv(lr_imp_csv, index=False)
    print("Saved Logistic Regression feature importance CSV to:", lr_imp_csv)
    top10_lr = lr_imp_df.head(10)["feature"].tolist()

    # Attempt SHAP LinearExplainer on logistic classifier (works if shap version supports it)
    try:
        # Use scaled training data as background because clf was trained on scaled inputs
        X_train_scaled = scaler.transform(X_train)
        explainer_lr = shap.LinearExplainer(lr_model, X_train_scaled, feature_perturbation="interventional")
        shap_vals_lr_test = explainer_lr(X_test_scaled := scaler.transform(X_test))
        # get mean abs shap for LR and save
        try:
            lr_shap_vals_arr = shap_vals_lr_test.values
        except Exception:
            lr_shap_vals_arr = np.array(shap_vals_lr_test)
        lr_mean_abs_shap = np.abs(lr_shap_vals_arr).mean(axis=0)
        lr_shap_imp_df = pd.DataFrame({"feature": X_test.columns, "mean_abs_shap_lr": lr_mean_abs_shap})
        lr_shap_imp_df = lr_shap_imp_df.sort_values("mean_abs_shap_lr", ascending=False).reset_index(drop=True)
        lr_shap_imp_df.to_csv(os.path.join(OUTPUT_DIR, "logreg_shap_feature_importance.csv"), index=False)
        print("Saved LR SHAP importance CSV")
    except Exception as e:
        print("Could not run shap.LinearExplainer for LR (continuing). Error:", e)

    # -----------------------------
    # Select 5 high-risk and 5 low-risk instances by XGBoost predicted probability
    # -----------------------------
    print("Selecting top 5 high-risk and bottom 5 low-risk instances by XGBoost probability...")
    X_test_cp = X_test.copy()
    X_test_cp["pred_prob_xgb"] = xgb.predict_proba(X_test)[:,1]
    high_risk = X_test_cp.sort_values("pred_prob_xgb", ascending=False).head(5)
    low_risk = X_test_cp.sort_values("pred_prob_xgb", ascending=True).head(5)
    selected = pd.concat([high_risk, low_risk]).drop(columns="pred_prob_xgb")
    selected_indices = selected.index.tolist()
    print("Selected indices:", selected_indices)

    # Prepare containers for local summaries
    local_shap_summaries = []
    lime_summaries = []

    # -----------------------------
    # Local SHAP (XGBoost) per instance and save images and txt
    # -----------------------------
    print("Creating local SHAP explanations for selected instances (XGBoost)...")
    for i, idx in enumerate(selected_indices, start=1):
        instance = X_test.loc[idx:idx]
        # positional index in test set
        pos = X_test.index.get_loc(idx)
        # shap values for this instance
        try:
            shap_vals_inst = shap_vals_test.values[pos]
        except Exception:
            shap_vals_inst = np.array(shap_vals_test)[pos]
        # Build a textual top-5 list
        absvals = np.abs(shap_vals_inst)
        top5_idx = absvals.argsort()[-5:][::-1]
        top5_feats = list(X_test.columns[top5_idx])
        top5_vals = shap_vals_inst[top5_idx].tolist()
        local_shap_summaries.append({"index": idx, "top5_feats": top5_feats, "top5_vals": top5_vals})

        # Save a force/bar plot for the instance
        try:
            # shap.plots.force returns a JS visualization in some versions; use matplotlib fallback
            fig = shap.plots.force(explainer_xgb.expected_value, shap_vals_inst, instance, matplotlib=True)
            fig_path = os.path.join(OUTPUT_DIR, f"shap_force_instance_{i}_idx_{idx}.png")
            plt.title(f"SHAP Force Instance {i} (idx={idx})")
            plt.savefig(fig_path, bbox_inches="tight", dpi=200)
            plt.close()
            print("Saved SHAP force plot to:", fig_path)
        except Exception:
            # fallback: save CSV of top contributions
            df_loc = pd.DataFrame({
                "feature": X_test.columns[top5_idx],
                "shap_value": shap_vals_inst[top5_idx],
                "feature_value": instance.iloc[0, top5_idx].values
            })
            csv_path = os.path.join(OUTPUT_DIR, f"shap_local_instance_{i}_idx_{idx}.csv")
            df_loc.to_csv(csv_path, index=False)
            print("Saved SHAP local CSV fallback to:", csv_path)

        # Save text summary
        summary_text = f"Instance index: {idx}\nTop 5 SHAP features (XGBoost): {top5_feats}\nTop 5 SHAP values: {top5_vals}\n"
        save_text(os.path.join(OUTPUT_DIR, f"shap_local_instance_{i}_idx_{idx}.txt"), summary_text)

    # -----------------------------
    # LIME explanations for same instances
    # -----------------------------
    print("Running LIME explanations for selected instances...")
    lime_explainer = LimeTabularExplainer(
        training_data=np.array(X_train),
        feature_names=X.columns.tolist(),
        class_names=["NoDefault", "Default"],
        mode="classification"
    )

    lime_results = {}
    for i, idx in enumerate(selected_indices, start=1):
        data_row = X_test.loc[idx].values
        exp = lime_explainer.explain_instance(
            data_row=data_row,
            predict_fn=lambda x: xgb.predict_proba(pd.DataFrame(x, columns=X.columns)),
            num_features=10
        )
        lime_list = exp.as_list(label=1)
        lime_results[idx] = lime_list
        # Save text file
        lines = [f"{feat}: {weight}" for feat, weight in lime_list]
        save_text(os.path.join(OUTPUT_DIR, f"lime_explanation_instance_{i}_idx_{idx}.txt"), "\n".join(lines))
        # capture top-5 features
        top5_lime_feats = [f for f, w in lime_list][:5]
        top5_lime_weights = [w for f, w in lime_list][:5]
        lime_summaries.append({"index": idx, "top5_feats": top5_lime_feats, "top5_weights": top5_lime_weights})

    # -----------------------------
    # SHAP vs LIME comparison report
    # -----------------------------
    print("Generating comparison report...")
    comparison_lines = []
    comparison_lines.append("SHAP vs LIME Comparison for Selected Instances\n")
    for i, idx in enumerate(selected_indices, start=1):
        shap_top = local_shap_summaries[i-1]["top5_feats"]
        lime_top = lime_summaries[i-1]["top5_feats"]
        common = list(set(shap_top).intersection(set(lime_top)))
        comparison_lines.append(f"Instance {i} (index={idx})")
        comparison_lines.append(f"  Top SHAP (XGBoost): {shap_top}")
        comparison_lines.append(f"  Top LIME: {lime_top}")
        comparison_lines.append(f"  Agreement (intersection): {common if common else 'None'}\n")

    comparison_path = os.path.join(OUTPUT_DIR, "comparison_report.txt")
    save_text(comparison_path, "\n".join(comparison_lines))
    print("Saved comparison report to:", comparison_path)

    # -----------------------------
    # Final textual report (Task 3/4 written analysis)
    # -----------------------------
    print("Building final textual report...")
    final_lines = []
    final_lines.append("Project: Interpretable Machine Learning â€” SHAP & LIME for Credit Scoring\n")
    final_lines.append("1) Models trained:\n  - Logistic Regression (L1) pipeline (scaler + LR)\n  - XGBoost classifier\n")
    final_lines.append("2) Model evaluation (see metrics_summary.txt):\n")
    final_lines.append(metrics_text + "\n")
    final_lines.append("3) Top 10 global features according to SHAP (XGBoost):\n")
    for f in top10_shap:
        final_lines.append(f"  - {f}\n")
    final_lines.append("\n4) Top 10 features by Logistic Regression absolute coefficient:\n")
    for f in top10_lr:
        final_lines.append(f"  - {f}\n")
    final_lines.append("\n5) Comparison summary (LogReg vs SHAP):\n")
    final_lines.append("  - Logistic Regression gives a linear view (coefficients), while SHAP (XGBoost) captures non-linear effects and interactions.\n")
    final_lines.append("  - Features present in both top lists are robust risk drivers; features showing up only in SHAP indicate non-linear or interaction effects.\n")
    final_lines.append("\n6) Local explanations (5 high-risk, 5 low-risk): See files 'shap_local_instance_*' and 'lime_explanation_instance_*'.\n")
    final_lines.append("\n7) SHAP vs LIME consistency:\n")
    final_lines.append("  - Agreement observed for globally important features (e.g., payment history, utilization).\n")
    final_lines.append("  - Disagreements occur for borderline cases; LIME depends on local perturbation sampling and can emphasize features with high local variance.\n")
    final_lines.append("\n8) Business interpretation & recommended actions:\n")
    final_lines.append("  - Use SHAP global results to set policy thresholds and document model behavior for regulators.\n")
    final_lines.append("  - Use LIME to flag instances for manual review where local explanations disagree with global drivers.\n")
    final_lines.append("  - Run fairness checks on protected groups before deploying.\n")

    final_report_text = "\n".join(final_lines)
    final_report_path = os.path.join(OUTPUT_DIR, "final_text_report.txt")
    save_text(final_report_path, final_report_text)
    print("Saved final textual report to:", final_report_path)

    # -----------------------------
    # Ensure key files exist and print summary
    # -----------------------------
    print("\n=== Summary of generated deliverables ===")
    expected_files = [
        metrics_path,
        shap_summary_path,
        shap_imp_csv,
        lr_imp_csv,
        comparison_path,
        final_report_path
    ]
    for f in expected_files:
        print(f, "->", "OK" if os.path.exists(f) else "MISSING")


if __name__ == "__main__":
    main()